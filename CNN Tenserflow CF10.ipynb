{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# The built-in dataset is loaded from the keras.datasets() as follows: \n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Checking the shape of the data\n",
    "print(train_images.shape, train_labels.shape)\n",
    "print(test_images.shape, test_labels.shape)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# In this section, we will focus solely on showing some sample images since we already know the proportion of each class in both the training and testing data.\n",
    "# \n",
    "# The helper function show_images() shows a total of 12 images by default and takes three main parameters:\n",
    "# \n",
    "# The training images\n",
    "# The class names\n",
    "# And the training labels.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(train_images,\n",
    "                class_names,\n",
    "                train_labels,\n",
    "                nb_samples = 12, nb_row = 4):\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(nb_samples):\n",
    "        plt.subplot(nb_row, nb_row, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "    plt.show()"
   ],
   "id": "2c80fb03958bc19d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "show_images(train_images, class_names, train_labels)"
   ],
   "id": "15c73b39bc173f3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data preprocessing\n",
    "# Prior to training the model, we need to normalize the pixel values of the data in the same range (e.g. 0 to 1). This is a common preprocessing step when dealing with images to ensure scale invariance, and faster convergence during the training.\n",
    "\n",
    "max_pixel_value = 255\n",
    "\n",
    "train_images = train_images / max_pixel_value\n",
    "test_images = test_images / max_pixel_value\n",
    "\n",
    "\n",
    "# labels are represented in a categorical format like cat, horse, bird, and so one. We need to convert them into a numerical format so that they can be easily processed by the neural network.\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels, len(class_names))\n",
    "test_labels = to_categorical(test_labels, len(class_names))"
   ],
   "id": "b50ceab5968337e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model architecture implementation\n",
    "# The next step is to implement the architecture of the network based on the previous description.\n",
    "# \n",
    "# First, we define the model using the Sequential() class, and each layer is added to the model with the add() function. \n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Variables\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "FILTER1_SIZE = 32\n",
    "FILTER2_SIZE = 64\n",
    "FILTER_SHAPE = (3, 3)\n",
    "POOL_SHAPE = (2, 2)\n",
    "FULLY_CONNECT_NUM = 128\n",
    "NUM_CLASSES = len(class_names)\n",
    "\n",
    "# Model architecture implementation\n",
    "model = Sequential()\n",
    "model.add(Conv2D(FILTER1_SIZE, FILTER_SHAPE, activation='relu', input_shape=INPUT_SHAPE))\n",
    "model.add(MaxPooling2D(POOL_SHAPE))\n",
    "model.add(Conv2D(FILTER2_SIZE, FILTER_SHAPE, activation='relu'))\n",
    "model.add(MaxPooling2D(POOL_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(FULLY_CONNECT_NUM, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))"
   ],
   "id": "7cff1498b42d7c97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model training\n",
    "# All the resources are finally available to configure and trigger the training of the model. This is done respectively with the compile() and fit() functions which takes the following parameters:\n",
    "# \n",
    "# The Optimizer is responsible for updating the modelâ€™s weights and biases. In our case, we are using the Adam optimizer.\n",
    "# The loss function is used to measure the misclassification errors, and we are using the Crosentropy().\n",
    "# Finally, the metrics is used to measure the performance of the model, and accuracy, precision, and recall will be displayed in our use case.\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "\n",
    "METRICS = metrics=['accuracy',\n",
    "                   Precision(name='precision'),\n",
    "                   Recall(name='recall')]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics = METRICS)\n",
    "\n",
    "# Train the model\n",
    "training_history = model.fit(train_images, train_labels,\n",
    "                             epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                             validation_data=(test_images, test_labels))"
   ],
   "id": "4a6e3adfd08e1bd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model evaluation\n",
    "# After the model training, we can compare its performance on both the training and testing datasets by plotting the above metrics using the show_performance_curve() helper function in two dimensions.\n",
    "# \n",
    "# The horizontal axis (x) is the number of epochs\n",
    "# The vertical one (y) is the underlying performance of the model.\n",
    "# The curve represents the value of the metrics at a specific epoch.\n",
    "# For better visualization, a vertical red line is drawn through the intersection of the training and validation performance values along with the optimal value.\n",
    "\n",
    "def show_performance_curve(training_result, metric, metric_label):\n",
    "\n",
    "    train_perf = training_result.history[str(metric)]\n",
    "    validation_perf = training_result.history['val_'+str(metric)]\n",
    "    intersection_idx = np.argwhere(np.isclose(train_perf,\n",
    "                                              validation_perf, atol=1e-2)).flatten()[0]\n",
    "    intersection_value = train_perf[intersection_idx]\n",
    "\n",
    "    plt.plot(train_perf, label=metric_label)\n",
    "    plt.plot(validation_perf, label = 'val_'+str(metric))\n",
    "    plt.axvline(x=intersection_idx, color='r', linestyle='--', label='Intersection')\n",
    "\n",
    "    plt.annotate(f'Optimal Value: {intersection_value:.4f}',\n",
    "                 xy=(intersection_idx, intersection_value),\n",
    "                 xycoords='data',\n",
    "                 fontsize=10,\n",
    "                 color='green')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.legend(loc='lower right')"
   ],
   "id": "509a024d743e5592"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "show_performance_curve(training_history, 'accuracy', 'accuracy')\n",
    "show_performance_curve(training_history, 'precision', 'precision')"
   ],
   "id": "c79f20d65eccf9ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# After training the model without any fine-tuning and pre-processing, we end up with:\n",
    "# \n",
    "# An accuracy score of 67.09%, meaning that the model correctly classifies 67% of the samples out of every 100 samples.\n",
    "# And, a precision of 76.55%, meaning that out of each 100 positive predictions, almost 77 of them are true positives, and the remaining 23 are false positives.\n",
    "# These scores are achieved respectively at the third and second epochs for accuracy and precision."
   ],
   "id": "b850961609e781b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "test_predictions = model.predict(test_images)\n",
    "\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "test_true_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "cm = confusion_matrix(test_true_labels, test_predicted_labels)\n",
    "\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "cmd.plot(include_values=True, cmap='viridis', ax=None, xticks_rotation='horizontal')\n",
    "plt.show()"
   ],
   "id": "a5f2c5aa8b415699"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
